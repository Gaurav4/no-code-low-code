## News Sentiment Classification


#### Context:
This dataset (FinancialPhraseBank) contains the sentiments for financial news headlines from the perspective of a retail investor.

#### Content:
The dataset contains two columns, `label` and `headline`. The label (sentiment) can be negative, neutral or positive.

#### ELECTRA-Small++:
`ELECTRA` (Efficiently Learning an Encoder that Classifies Token Replacement Accurately) is a method for self-supervised language representation learning. It can be used to pre-train **transformer networks** using relatively little compute. ELECTRA models are trained to distinguish "real" input tokens vs "fake" input tokens generated by another neural network, similar to the discriminator of a GAN (Generative Adversarial Networks).

`ELECTRA` replaces the **MLM (Masked Language Model)** of `BERT` with **Replaced Token Detection (RTD)**, which looks to be more efficient and produces better results. In BERT, the input is replaced by some tokens with `[MASK]` and then a model is trained to reconstruct the original tokens.

In `ELECTRA`, instead of masking the input, the approach corrupts it by replacing some input tokens with plausible alternatives sampled from a small generator network. Then, instead of training a model that predicts the original identities of the corrupted tokens, a discriminative model is trained that predicts whether each token in the corrupted input was replaced by a generator sample or not. This new pre-training task is more efficient than MLM because the model learns from all input tokens rather than just the small subset that was masked out.

#### SageMaker Jumpstart:
`Amazon SageMaker JumpStart` is a capability of Amazon SageMaker that accelerates your machine learning workflows with one-click access to popular model collections (also known as **model zoos**), and to end-to-end solutions that solve common use cases.

#### Downstream Task:
Multi-class Classification 

#### List of pre-trained language models available for fine-tuning:
* BERT Base Uncased
* BERT Base Cased
* BERT Base Multilingual Cased
* BERT Base MEDLINE/PubMed

* BERT Large Cased
* BERT Large Cased Whole Word Masking
* BERT Large Uncased Whole Word Masking

* ELECTRA-Base++

* **ELECTRA-Small++**


